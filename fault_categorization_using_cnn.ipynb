{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fault_categorization_using_cnn.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Jeevan-J/Python_Funcode/blob/Python/fault_categorization_using_cnn.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "Ep38cbjdg8CE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "15080d75-8334-4bd3-effa-a4015ebce24c"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "!pip install -q keras\n",
        "!pip install xlrd\n",
        "!pip install xlsxwriter"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n",
            "Collecting xlrd\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/e6/e95c4eec6221bfd8528bcc4ea252a850bffcc4be88ebc367e23a1a84b0bb/xlrd-1.1.0-py2.py3-none-any.whl (108kB)\n",
            "\u001b[K    100% |████████████████████████████████| 112kB 6.5MB/s \n",
            "\u001b[?25hInstalling collected packages: xlrd\n",
            "Successfully installed xlrd-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-1_y5b-Ibg3G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import itertools\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import keras\n",
        "\n",
        "\n",
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for datasets.\n",
        "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip().lower()\n",
        "\n",
        "\n",
        "def load_data_and_labels():\n",
        "    \"\"\"\n",
        "    Loads polarity data from files, splits the data into words and generates labels.\n",
        "    Returns split sentences and labels.\n",
        "    \"\"\"\n",
        "    #Load data from xlsx\n",
        "    df = pd.read_excel('drive/Colab Notebooks/Wuling TAC (technical assistant center)  case list.xlsx', sheet_name='信息反馈汇总');\n",
        "\n",
        "    fault_labels = df['fault category']\n",
        "    fault_labels = list(fault_labels[1:])\n",
        "    fault_description = df['fault description (voice of customer)']\n",
        "    fault_description = list(fault_description[1:])\n",
        "    \n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(fault_labels)\n",
        "    no_of_faults = len(list(encoder.classes_))\n",
        "    y = encoder.transform(fault_labels)\n",
        "    y = keras.utils.to_categorical(y,num_classes=None)\n",
        "    \n",
        "    x_text = [s.strip() for s in fault_description]\n",
        "    #x_text = [clean_str(sentence) for sentence in x_text]\n",
        "    return [x_text, y, no_of_faults]\n",
        "\n",
        "\n",
        "def pad_sentences(sentences, padding_word=\"P\"):\n",
        "    \"\"\"\n",
        "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
        "    Returns padded sentences.\n",
        "    \"\"\"\n",
        "    sequence_length = max(len(x) for x in sentences)\n",
        "    padded_sentences = []\n",
        "    for i in range(len(sentences)):\n",
        "        sentence = str(sentences[i])\n",
        "        num_padding = sequence_length - len(sentence)\n",
        "        new_sentence = sentence + padding_word * num_padding\n",
        "        padded_sentences.append(new_sentence)\n",
        "    return padded_sentences\n",
        "\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    \"\"\"\n",
        "    Builds a vocabulary mapping from word to index based on the sentences.\n",
        "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
        "    \"\"\"\n",
        "    # Build vocabulary\n",
        "    word_counts = Counter(itertools.chain(*sentences))\n",
        "    # Mapping from index to word\n",
        "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
        "    vocabulary_inv = list(sorted(vocabulary_inv))\n",
        "    # Mapping from word to index\n",
        "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
        "    return [vocabulary, vocabulary_inv]\n",
        "\n",
        "\n",
        "def build_input_data(sentences, labels, vocabulary):\n",
        "    \"\"\"\n",
        "    Maps sentences and labels to vectors based on a vocabulary.\n",
        "    \"\"\"\n",
        "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
        "    y = np.array(labels)\n",
        "    return [x, y]\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    Loads and preprocessed data for the dataset.\n",
        "    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
        "    \"\"\"\n",
        "    # Load and preprocess data\n",
        "    sentences, labels, output_classes = load_data_and_labels()\n",
        "    #print(str(sentences[1]))\n",
        "    sentences_padded = pad_sentences(sentences)\n",
        "    vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
        "    x, y = build_input_data(sentences_padded, labels, vocabulary)\n",
        "    #print(x)\n",
        "    return [x, y, vocabulary, vocabulary_inv,output_classes]\n",
        "  \n",
        "def rebuild(encoded_labels):\n",
        "    df = pd.read_excel('drive/Colab Notebooks/Wuling TAC (technical assistant center)  case list.xlsx', sheet_name='信息反馈汇总');\n",
        "\n",
        "    fault_labels = df['fault category']\n",
        "    fault_labels = list(fault_labels[1:])\n",
        "    \n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(fault_labels)\n",
        "    fault_description = df['fault description (voice of customer)']\n",
        "    fault_description = list(fault_description[1:])\n",
        "    \n",
        "    pred_labels = encoder.inverse_transform(encoded_labels)\n",
        "    return pred_labels,fault_description"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qF163_uHdbHJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "4f113849-a42c-496a-c86d-17250a488581"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
        "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adam,SGD\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from data_helpers import load_data\n",
        "\n",
        "print('Loading data')\n",
        "x, y, vocabulary, vocabulary_inv, output_classes = load_data()\n",
        "# x = x.reshape(len(x),len(x[1]))\n",
        "# x.shape -> (10662, 56)\n",
        "# y.shape -> (10662, 2)\n",
        "# len(vocabulary) -> 18765\n",
        "# len(vocabulary_inv) -> 18765\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# X_train.shape -> (8529, 56)\n",
        "# y_train.shape -> (8529, 2)\n",
        "# X_test.shape -> (2133, 56)\n",
        "# y_test.shape -> (2133, 2)\n",
        "\n",
        "\n",
        "sequence_length = len(x[1]) # 56\n",
        "vocabulary_size = len(voucabulary_inv) # 18765\n",
        "embedding_dim = 256\n",
        "filter_sizes = [3,4,5]\n",
        "num_filters = 512\n",
        "drop = 0.5\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "# this returns a tensor\n",
        "print(\"Creating Model...\")\n",
        "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
        "embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
        "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n",
        "\n",
        "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "\n",
        "maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
        "maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
        "maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
        "\n",
        "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
        "flatten = Flatten()(concatenated_tensor)\n",
        "dropout = Dropout(drop)(flatten)\n",
        "output = Dense(units=output_classes, activation='softmax')(dropout)\n",
        "\n",
        "# this creates a model that includes\n",
        "model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "checkpoint = ModelCheckpoint('drive/Colab Notebooks/weights_sgd.{epoch:03d}-{val_acc:.4f}.hdf5', save_best_only=True, mode='auto')\n",
        "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print(\"Traning Model...\")\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=[checkpoint], validation_data=(X_test, y_test))  # starts training\n",
        "model.save_weights('weights.h5')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data\n",
            "Creating Model...\n",
            "Traning Model...\n",
            "Train on 30797 samples, validate on 7700 samples\n",
            "Epoch 1/10\n",
            "16544/30797 [===============>..............] - ETA: 3:03 - loss: 0.7726 - acc: 0.7171"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "30797/30797 [==============================] - 436s 14ms/step - loss: 0.7163 - acc: 0.7377 - val_loss: 0.6509 - val_acc: 0.7687\n",
            "Epoch 2/10\n",
            " 8832/30797 [=======>......................] - ETA: 4:42 - loss: 0.6018 - acc: 0.7746"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "30797/30797 [==============================] - 435s 14ms/step - loss: 0.6074 - acc: 0.7782 - val_loss: 0.6774 - val_acc: 0.7422\n",
            "Epoch 3/10\n",
            " 5920/30797 [====>.........................] - ETA: 5:18 - loss: 0.5318 - acc: 0.8061"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "30797/30797 [==============================] - 434s 14ms/step - loss: 0.5606 - acc: 0.7969 - val_loss: 0.6263 - val_acc: 0.7769\n",
            "Epoch 4/10\n",
            " 4800/30797 [===>..........................] - ETA: 5:33 - loss: 0.4919 - acc: 0.8246"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "30797/30797 [==============================] - 434s 14ms/step - loss: 0.5167 - acc: 0.8149 - val_loss: 0.6571 - val_acc: 0.7836\n",
            "Epoch 5/10\n",
            " 4384/30797 [===>..........................] - ETA: 5:36 - loss: 0.4474 - acc: 0.8410"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "30797/30797 [==============================] - 433s 14ms/step - loss: 0.4728 - acc: 0.8289 - val_loss: 0.6629 - val_acc: 0.7753\n",
            "Epoch 6/10\n",
            " 4224/30797 [===>..........................] - ETA: 5:40 - loss: 0.3857 - acc: 0.8591"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "30797/30797 [==============================] - 434s 14ms/step - loss: 0.4321 - acc: 0.8443 - val_loss: 0.6808 - val_acc: 0.7839\n",
            "Epoch 7/10\n",
            " 4160/30797 [===>..........................] - ETA: 5:41 - loss: 0.3508 - acc: 0.8788"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "30797/30797 [==============================] - 434s 14ms/step - loss: 0.3917 - acc: 0.8609 - val_loss: 0.7499 - val_acc: 0.7855\n",
            "Epoch 8/10\n",
            " 4128/30797 [===>..........................] - ETA: 5:42 - loss: 0.3106 - acc: 0.8917"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "30797/30797 [==============================] - 434s 14ms/step - loss: 0.3609 - acc: 0.8725 - val_loss: 0.7736 - val_acc: 0.7865\n",
            "Epoch 9/10\n",
            " 4128/30797 [===>..........................] - ETA: 5:41 - loss: 0.2663 - acc: 0.9048"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "30797/30797 [==============================] - 434s 14ms/step - loss: 0.3329 - acc: 0.8834 - val_loss: 0.8527 - val_acc: 0.7747\n",
            "Epoch 10/10\n",
            " 4096/30797 [==>...........................] - ETA: 5:41 - loss: 0.2847 - acc: 0.8962"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "30797/30797 [==============================] - 434s 14ms/step - loss: 0.3124 - acc: 0.8905 - val_loss: 0.8749 - val_acc: 0.7749\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dTQ0TVwQFqfl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
        "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adam,SGD\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "#from data_helpers import load_data\n",
        "\n",
        "print('Loading data')\n",
        "x, y, vocabulary, vocabulary_inv, output_classes = load_data()\n",
        "\n",
        "\n",
        "sequence_length = len(x[1]) # 56\n",
        "vocabulary_size = len(vocabulary_inv) # 18765\n",
        "embedding_dim = 256\n",
        "filter_sizes = [3,4,5]\n",
        "num_filters = 512\n",
        "drop = 0.5\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "# this returns a tensor\n",
        "print(\"Creating Model...\")\n",
        "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
        "embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
        "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n",
        "\n",
        "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "\n",
        "maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
        "maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
        "maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
        "\n",
        "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
        "flatten = Flatten()(concatenated_tensor)\n",
        "dropout = Dropout(drop)(flatten)\n",
        "output = Dense(units=output_classes, activation='softmax')(dropout)\n",
        "\n",
        "# this creates a model that includes\n",
        "model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print(\"Predicting Model...\")\n",
        "model.load_weights('drive/Colab Notebooks/weights_sgd.003-0.7769.hdf5')\n",
        "y_pred = np.rint(model.predict(x))\n",
        "print(accuracy_score(y, y_pred))\n",
        "\n",
        "y_pred1 = np.argmax(y_pred,axis=1)\n",
        "Pred_fault_labels,fault_descriptions = rebuild(y_pred1)\n",
        "writer = pd.ExcelWriter('drive/Colab Notebooks/output.xlsx', engine='xlsxwriter')\n",
        "df1 = pd.DataFrame({'fault_descriptions': fault_descriptions})\n",
        "df1.to_excel(writer, sheet_name='Sheet1')\n",
        "df2 = pd.DataFrame({'Predicted_fault_labels': Pred_fault_labels})\n",
        "df2.to_excel(writer, sheet_name='Sheet1', startcol = 4)\n",
        "writer.save()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}